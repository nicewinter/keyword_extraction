{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction APIs and Packages   \n",
    "\n",
    "- Keyword Extraction is the automated procedure of finding the most relevant and important words and phrases from text.  \n",
    "- A Comprehensive Guide to Keyword Extraction analysis: what it is, how it works, use cases:    \n",
    "https://monkeylearn.com/keyword-extraction/![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAKE NLTK\n",
    "\n",
    "- RAKE NLTK is a specific Python implementation of the **Rapid Automatic Keyword Extraction (RAKE) algorithm** that uses NLTK under the hood. This makes it easier to extend and perform other text analysis tasks.  \n",
    "- Github repo: https://github.com/csurfer/rake-nltk  \n",
    "- This is a python implementation of the algorithm as mentioned in paper: **Automatic keyword extraction from individual documents** by Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley  \n",
    "- There are some rather popular implementations out there, in python(aneesha/RAKE) and node(waseem18/node-rake) but neither seemed to use the power of NLTK. By making NLTK an integral part of the implementation its author get the flexibility and power to extend it in other creative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make better business decisions',\n",
       " 'keyword extraction allows business',\n",
       " 'teams many hours',\n",
       " 'keyword extraction models',\n",
       " 'given text without',\n",
       " 'keyword extraction',\n",
       " 'single line',\n",
       " 'manual processing',\n",
       " 'key phrases',\n",
       " 'important words',\n",
       " 'important keywords',\n",
       " 'customer review',\n",
       " 'big data',\n",
       " 'best thing',\n",
       " 'best describe',\n",
       " 'also provides',\n",
       " 'actually read',\n",
       " 'actionable insights',\n",
       " 'obtain insights',\n",
       " 'text',\n",
       " 'obtain',\n",
       " 'use',\n",
       " 'topics',\n",
       " 'talking',\n",
       " 'sift',\n",
       " 'set',\n",
       " 'seconds',\n",
       " 'saving',\n",
       " 'implement',\n",
       " 'help',\n",
       " 'g',\n",
       " 'easy',\n",
       " 'e',\n",
       " 'customers',\n",
       " 'capture']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses stopwords for english from NLTK, and all puntuation characters by default\n",
    "r = Rake()\n",
    "\n",
    "# raw text to extract keywords\n",
    "myText = '''\n",
    "Keyword extraction allows business to sift through big data to capture the most important words that best \n",
    "describe the text (e.g. customer review) in just seconds, obtain insights about the topics that your customers are \n",
    "talking about while saving your teams many hours of manual processing. \n",
    "It also provides you with actionable insights that you can use to make better business decisions.\n",
    "The best thing about keyword extraction models is that they are easy to set up and implement.\n",
    "Keyword extraction can help you obtain the most important keywords or key phrases from a given text without having to actually read a single line.\n",
    "'''\n",
    "\n",
    "# Extraction given the text.\n",
    "r.extract_keywords_from_text(myText)\n",
    "\n",
    "# Extraction given the list of strings where each string is a sentence.\n",
    "#r.extract_keywords_from_sentences(<list of sentences>)\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest.\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "We can see RAKE-NLTK nicely extracts keywords and keyphrases from the sample text. Following shows ranking score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16.0, 'make better business decisions'),\n",
       " (14.0, 'keyword extraction allows business'),\n",
       " (9.0, 'teams many hours'),\n",
       " (9.0, 'keyword extraction models'),\n",
       " (8.0, 'given text without'),\n",
       " (6.0, 'keyword extraction'),\n",
       " (4.0, 'single line'),\n",
       " (4.0, 'manual processing'),\n",
       " (4.0, 'key phrases'),\n",
       " (4.0, 'important words'),\n",
       " (4.0, 'important keywords'),\n",
       " (4.0, 'customer review'),\n",
       " (4.0, 'big data'),\n",
       " (4.0, 'best thing'),\n",
       " (4.0, 'best describe'),\n",
       " (4.0, 'also provides'),\n",
       " (4.0, 'actually read'),\n",
       " (4.0, 'actionable insights'),\n",
       " (3.5, 'obtain insights'),\n",
       " (2.0, 'text'),\n",
       " (1.5, 'obtain'),\n",
       " (1.0, 'use'),\n",
       " (1.0, 'topics'),\n",
       " (1.0, 'talking'),\n",
       " (1.0, 'sift'),\n",
       " (1.0, 'set'),\n",
       " (1.0, 'seconds'),\n",
       " (1.0, 'saving'),\n",
       " (1.0, 'implement'),\n",
       " (1.0, 'help'),\n",
       " (1.0, 'g'),\n",
       " (1.0, 'easy'),\n",
       " (1.0, 'e'),\n",
       " (1.0, 'customers'),\n",
       " (1.0, 'capture')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get keyword phrases ranked highest to lowest with scores.\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Debugging Setup\n",
    "If you see a stopwords error, it means that you do not have the corpus stopwords downloaded from NLTK. You can download it using command below.\n",
    "\n",
    "python -c \"import nltk; nltk.download('stopwords')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn with TF-IDF    \n",
    "- Use Scikit-learn to extract keywords with TF-IDF (Score)    \n",
    "- An excellent tutorial: \n",
    "https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/  \n",
    "- Tutorial repo with stackoverflow data samples: https://github.com/kavgan/nlp-in-practice/tree/master/tf-idf  \n",
    "- Tutorial notebook: https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stackoverflow as sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " accepted_answer_id          float64\n",
      "answer_count                  int64\n",
      "body                         object\n",
      "comment_count                 int64\n",
      "community_owned_date         object\n",
      "creation_date                object\n",
      "favorite_count              float64\n",
      "id                            int64\n",
      "last_activity_date           object\n",
      "last_edit_date               object\n",
      "last_editor_display_name     object\n",
      "last_editor_user_id         float64\n",
      "owner_display_name           object\n",
      "owner_user_id               float64\n",
      "post_type_id                  int64\n",
      "score                         int64\n",
      "tags                         object\n",
      "title                        object\n",
      "view_count                    int64\n",
      "dtype: object\n",
      "Number of questions,columns= (20000, 19)\n",
      "stackoverflow training dataset shape:(20000, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read json into a dataframe\n",
    "# data source acknowledgement: https://github.com/kavgan/nlp-in-practice/tree/master/tf-idf/data\n",
    "df_idf=pd.read_json(\"./stackoverflow-data-idf.json\",lines=True)\n",
    "\n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "print(\"Number of questions,columns=\",df_idf.shape)\n",
    "print(f'stackoverflow training dataset shape:{df_idf.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>community_owned_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>last_editor_display_name</th>\n",
       "      <th>last_editor_user_id</th>\n",
       "      <th>owner_display_name</th>\n",
       "      <th>owner_user_id</th>\n",
       "      <th>post_type_id</th>\n",
       "      <th>score</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;I have a public class that contains a priva...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-27 20:19:13.563 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4821394</td>\n",
       "      <td>2011-01-27 20:21:37.59 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>163534.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c#|serialization|xml-serialization</td>\n",
       "      <td>Serializing a private struct - Can it be done?</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3367943.0</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;p&gt;I have the following HTML:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;cod...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-30 00:01:50.9 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3367882</td>\n",
       "      <td>2012-05-10 14:16:05.143 UTC</td>\n",
       "      <td>2012-05-10 14:16:05.143 UTC</td>\n",
       "      <td></td>\n",
       "      <td>44390.0</td>\n",
       "      <td></td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>css|overflow|css-float|crop</td>\n",
       "      <td>How do I prevent floated-right content from ov...</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt;I'm trying to run a shell script with gradl...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-07-28 16:30:18.28 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31682135</td>\n",
       "      <td>2015-07-28 16:32:15.117 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>1299158.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bash|shell|android-studio|gradle</td>\n",
       "      <td>Gradle command line</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accepted_answer_id  answer_count  \\\n",
       "0                 NaN             1   \n",
       "1           3367943.0             2   \n",
       "2                 NaN             0   \n",
       "\n",
       "                                                body  comment_count  \\\n",
       "0  <p>I have a public class that contains a priva...              0   \n",
       "1  <p>I have the following HTML:</p>\\n\\n<pre><cod...              2   \n",
       "2  <p>I'm trying to run a shell script with gradl...              2   \n",
       "\n",
       "  community_owned_date                creation_date  favorite_count        id  \\\n",
       "0                  NaN  2011-01-27 20:19:13.563 UTC             NaN   4821394   \n",
       "1                  NaN    2010-07-30 00:01:50.9 UTC             0.0   3367882   \n",
       "2                  NaN   2015-07-28 16:30:18.28 UTC             NaN  31682135   \n",
       "\n",
       "            last_activity_date               last_edit_date  \\\n",
       "0   2011-01-27 20:21:37.59 UTC                          NaN   \n",
       "1  2012-05-10 14:16:05.143 UTC  2012-05-10 14:16:05.143 UTC   \n",
       "2  2015-07-28 16:32:15.117 UTC                          NaN   \n",
       "\n",
       "  last_editor_display_name  last_editor_user_id owner_display_name  \\\n",
       "0                                           NaN                      \n",
       "1                                       44390.0                      \n",
       "2                                           NaN                      \n",
       "\n",
       "   owner_user_id  post_type_id  score                                tags  \\\n",
       "0       163534.0             1      0  c#|serialization|xml-serialization   \n",
       "1         1190.0             1      2         css|overflow|css-float|crop   \n",
       "2      1299158.0             1      1    bash|shell|android-studio|gradle   \n",
       "\n",
       "                                               title  view_count  \n",
       "0     Serializing a private struct - Can it be done?         296  \n",
       "1  How do I prevent floated-right content from ov...        4121  \n",
       "2                                Gradle command line         259  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that this stackoverflow dataset contains 19 fields including post title, body, tags, dates and other metadata which we don't quite need for this tutorial. **What we are mostly interested in for this tutorial is the body and title which is our source of text.** We will now create a new field 'text' that combines both body and title so we have it in one field. We will also print the second text entry in our new field just to see what the text looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text) #e.g. replace </p> with <>\n",
    "    \n",
    "    # remove all special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_idf['text'] = df_idf['title'] + df_idf['body'] #generate a new column 'text' combining both 'title' and 'body'\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gradle command line<p>I\\'m trying to run a shell script with gradle. I currently have something like this</p>\\n\\n<pre><code>def test = project.tasks.create(\"test\", Exec) {\\n    commandLine \\'bash\\', \\'-c\\', \\'bash C:/my file dir/script.sh\\'\\n}\\n</code></pre>\\n\\n<p>The problem is that I cannot run this script because i have spaces in my dir name. I have tried everything e.g: </p>\\n\\n<pre><code>commandLine \\'bash\\', \\'-c\\', \\'bash C:/my file dir/script.sh\\'.tokenize() \\ncommandLine \\'bash\\', \\'-c\\', [\\'bash\\', \\'C:/my file dir/script.sh\\']\\ncommandLine \\'bash\\', \\'-c\\', new StringBuilder().append(\\'bash\\').append(\\'C:/my file dir/script.sh\\')\\ncommandLine \\'bash\\', \\'-c\\', \\'bash \"C:/my file dir/script.sh\"\\'\\nFile dir = file(\\'C:/my file dir/script.sh\\')\\ncommandLine \\'bash\\', \\'-c\\', \\'bash \\' + dir.getAbsolutePath();\\n</code></pre>\\n\\n<p>Im using windows7 64bit and if I use a path without spaces the script runs perfectly, therefore the only issue as I can see is how gradle handles spaces.</p>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show its original 'body' content\n",
    "df_idf['title'][2] + df_idf['body'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradle command line i m trying to run a shell script with gradle i currently have something like this def test project tasks create test exec commandline bash c bash c my file dir script sh the problem is that i cannot run this script because i have spaces in my dir name i have tried everything e g commandline bash c bash c my file dir script sh tokenize commandline bash c bash c my file dir script sh commandline bash c new stringbuilder append bash append c my file dir script sh commandline bash c bash c my file dir script sh file dir file c my file dir script sh commandline bash c bash dir getabsolutepath im using windows bit and if i use a path without spaces the script runs perfectly therefore the only issue as i can see is how gradle handles spaces '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show a sample of cleaned 'text' column\n",
    "df_idf['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, doesn't look very pretty with all the html in there, but that's the point. Even in such a mess we can extract some great stuff out of this. While you can eliminate all code from the text, we will keep the code sections for this tutorial for the sake of simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the TF - IDF  \n",
    "#### TF - Term Frequency\n",
    "#### Using Sklearn's CountVectorizer to create a vocabulary and generate word counts\n",
    "The next step is to start the counting process. We can use the CountVectorizer to create a vocabulary from all the text in our dataset df_idf['text'] and generate counts for each row in df_idf['text']. The result of the last two lines is a sparse matrix representation of the counts, **meaning each column represents a word (a.k.a. term/token) in the vocabulary and each row represents a document/record in our dataset where the values are the word counts (i.e. TF)**. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: docs contains a list of text strings for model training with 20000 samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/houj/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stopwords if you want to use custom stopwords \n",
    "#stopwords=get_stop_words(\"resources/stopwords.txt\")\n",
    "stopwords=get_stop_words(\"./stopwords.txt\")\n",
    "\n",
    "#get the text column (note: docs contains a list of text strings for model training!)\n",
    "docs=df_idf['text'].tolist() #a list of text strings\n",
    "print(f'Note: docs contains a list of text strings for model training with {len(docs)} samples!')\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#max_df=0.85: ignore stopwords and words that appear in 85% of documents, a tunable hyperparameter, normally [0.7,1.0], This parameter is ignored if vocabulary is not None! \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs) #output is a sparse matrix representation of the word counts: N_docs x M_words_in_Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the shape of the resulting vector. Notice that the shape below is (20000,124901) because we have 20,000 documents in our dataset (the rows) and the vocabulary size is 124901 meaning we have 124k unique words (the columns) in our dataset minus the stopwords. **(Optional) in some of the text mining applications, such as clustering and text classification we may limit the size of the vocabulary. It's really easy to do this by setting max_features=vocab_size when instantiating CountVectorizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords numbers:  752\n",
      "word count vector shape:(20000, 124901)\n"
     ]
    }
   ],
   "source": [
    "print('stopwords numbers: ', len(stopwords))\n",
    "print(f'word count vector shape:{word_count_vector.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our vocabulary size to 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/houj/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 7852)\t1\n",
      "  (0, 6761)\t3\n",
      "  (0, 8520)\t5\n",
      "  (0, 6888)\t3\n",
      "  (0, 1351)\t1\n",
      "  (0, 1729)\t2\n",
      "  (0, 6846)\t1\n",
      "  (0, 8498)\t1\n",
      "  (0, 7848)\t3\n",
      "  (0, 631)\t1\n",
      "  (0, 8486)\t1\n",
      "  (0, 2411)\t1\n",
      "  (0, 9431)\t1\n",
      "  (0, 9909)\t1\n",
      "  (0, 2831)\t1\n",
      "  (0, 7651)\t1\n",
      "  (0, 6065)\t1\n",
      "  (0, 9212)\t1\n",
      "  (0, 7849)\t1\n",
      "  (0, 2497)\t2\n",
      "  (0, 5744)\t1\n",
      "  (0, 9650)\t1\n",
      "  (0, 4689)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(word_count_vector))\n",
    "print(word_count_vector[0,:]) #a sparse matrix representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at 10 words from our vocabulary. Sweet, these are mostly programming related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serializing',\n",
       " 'private',\n",
       " 'struct',\n",
       " 'public',\n",
       " 'class',\n",
       " 'contains',\n",
       " 'properties',\n",
       " 'string',\n",
       " 'serialize',\n",
       " 'attempt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the vocabulary by using `get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customization',\n",
       " 'customize',\n",
       " 'customized',\n",
       " 'customlog',\n",
       " 'customview',\n",
       " 'cut',\n",
       " 'cv',\n",
       " 'cv_',\n",
       " 'cval',\n",
       " 'cvc',\n",
       " 'cw',\n",
       " 'cwd',\n",
       " 'cx',\n",
       " 'cx_oracle',\n",
       " 'cxf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.get_feature_names())[2000:2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(list(cv.vocabulary_.keys())), len(list(cv.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF \n",
    "#### Use Sklearn's TfidfTransformer to Compute Inverse Document Frequency (IDF)  \n",
    "\n",
    "In the code below, we are essentially taking the TF sparse matrix from CountVectorizer to generate the IDF when you invoke `fit`. **An extremely important point to note here is that the IDF should be based on a large enough corpora and should be representative of texts you would be using to extract keywords (i.e. use similar and relevant text/docs in the same/similar domain as training data!).** \n",
    "\n",
    "To understand why IDF should be based on a fairly large collection, please read this page from Standford's IR - Information Retrieval online book (https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the IDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.37717703,  9.80492526,  9.51724319, ...,  8.82409601,\n",
       "       10.21039037,  9.51724319])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: idf is Global, so just one value for each word/token in the vocabulary, regardless of how many records! \n",
    "Say, you have 100 records and if one word/token only appears in one record, its Document Frequency (DF) is 1/100, thus its IDF, Inverse DF is 100. For another work/token which appears in 50 out of 100 records, its IDF will be 2. So the former word/token has much higher IDF score than the latter as the fact that it only presents in 1 out of 100 records contains much more info itself and it's very unique in that record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing TF-IDF and Extracting Keywords  \n",
    "\n",
    "- Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top keywords.  \n",
    "- In this example, we will extract top keywords for the questions in stackoverflow test sample file: `data/stackoverflow-test.json`. This data file has 500 questions with fields identical to that of training data file: `data/stackoverflow-data-idf.json` as we saw above. We will start by reading our test file, extracting the necessary fields (title and body) and get the texts into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackoverflow testing dataset shape:(500, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# read test docs into a dataframe and concatenate title and body\n",
    "df_test=pd.read_json(\"./stackoverflow-test.json\",lines=True)\n",
    "df_test['text'] = df_test['title'] + df_test['body']\n",
    "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
    "print(f'stackoverflow testing dataset shape:{df_test.shape}')\n",
    "\n",
    "# get test docs into a list\n",
    "docs_test=df_test['text'].tolist()\n",
    "docs_title=df_test['title'].tolist()\n",
    "docs_body=df_test['body'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In following, define utility functions to sort and get top n keywords/keyphrases for each document/record, according to their tf-idf scores\n",
    "#### Rationale: if a word is more unique globally among all docs and more frequently appearing in a given document, its tf-idf score is higher thus should be recognised as top-ranking keyword to be extracted! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data) #tuples of (idx_word_in_vocabulary, word_tf_idf_score)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)#sort tuples firstly according to word_tf_idf_score in descending order\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "#a simplified and better version done by myself\n",
    "def extract_topn_from_vector2(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    #results is a look-up dict\n",
    "    results = {}\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        results[fname] = round(score,3) #a dict: key:value,where key is word and value is word's tf_idf score\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract keywords from test data documents/records\n",
    "- The next step is to compute the tf-idf value for a given document in our test set by invoking `tfidf_transformer.transform(...)`. This generates a vector of tf-idf scores.  \n",
    "- Next, we sort the words in the vector in descending order of tf-idf values and then iterate over to extract the top-n items with the corresponding feature (i.e. token/words) names, In the example below, we are extracting keywords for the first document in our test set.\n",
    "- Note: the `sort_coo(...)` method essentially sorts the values in the vector while preserving the column index. Once you have the column index then its really easy to look-up the corresponding word value as you would see in `extract_topn_from_vector2(...)` where we do `fname = feature_names[idx]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Integrate War-Plugin for m2eclipse into Eclipse Project\n",
      "\n",
      "=====Body=====\n",
      "<p>I set up a small web project with JSF and Maven. Now I want to deploy on a Tomcat server. Is there a possibility to automate that like a button in Eclipse that automatically deploys the project to Tomcat?</p>\n",
      "\n",
      "<p>I read about a the <a href=\"http://maven.apache.org/plugins/maven-war-plugin/\" rel=\"nofollow noreferrer\">Maven War Plugin</a> but I couldn't find a tutorial how to integrate that into my process (eclipse/m2eclipse).</p>\n",
      "\n",
      "<p>Can you link me to help or try to explain it. Thanks.</p>\n",
      "\n",
      "===Keywords===\n",
      "eclipse 0.593\n",
      "war 0.317\n",
      "integrate 0.281\n",
      "maven 0.273\n",
      "tomcat 0.27\n",
      "project 0.239\n",
      "plugin 0.214\n",
      "automate 0.157\n",
      "jsf 0.152\n",
      "possibility 0.146\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_test[0] # get the first record from a list of text strings\n",
    "\n",
    "#generate tf-idf score vector for the given document - a key command to calculate tf-idf score! \n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10, custom it to any top N value\n",
    "top_n=10\n",
    "#keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "keywords=extract_topn_from_vector2(feature_names,sorted_items,top_n)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(docs_title[0])\n",
    "print(\"\\n=====Body=====\")\n",
    "print(docs_body[0])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Result Observation\n",
    "From the keywords above, the top keywords actually make sense, it talks about eclipse, maven, integrate, war and tomcat which are all unique to this specific question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All-in-one, I defined utility functions to freely select and test any document samples as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the common code into several methods - my self-define utility funcs here\n",
    "def get_keywords(idx): #idx: doc index\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector2(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(docs_title[idx])\n",
    "    print(\"\\n=====Body=====\")\n",
    "    print(docs_body[idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at keywords generated for a much longer question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "SQL Import Wizard - Error\n",
      "\n",
      "=====Body=====\n",
      "<p>I have a CSV file that I'm trying to import into SQL Management Server Studio.</p>\n",
      "\n",
      "<p>In Excel, the column giving me trouble looks like this:\n",
      "<a href=\"https://i.stack.imgur.com/pm0uS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pm0uS.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>Tasks > import data > Flat Source File > select file</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/G4b6I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/G4b6I.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>I set the data type for this column to DT_NUMERIC, adjust the DataScale to 2 in order to get 2 decimal places, but when I click over to Preview, I see that it's clearly not recognizing the numbers appropriately:</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/NZhiQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NZhiQ.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>The column mapping for this column is set to type = decimal; precision 18; scale 2.</p>\n",
      "\n",
      "<p>Error message: Data Flow Task 1: Data conversion failed. The data conversion for column \"Amount\" returned status value 2 and status text \"The value could not be converted because of a potential loss of data.\".\n",
      " (SQL Server Import and Export Wizard)</p>\n",
      "\n",
      "<p>Can someone identify where I'm going wrong here?  Thanks!</p>\n",
      "\n",
      "===Keywords===\n",
      "column 0.365\n",
      "import 0.286\n",
      "data 0.283\n",
      "wizard 0.27\n",
      "decimal 0.227\n",
      "conversion 0.224\n",
      "sql 0.217\n",
      "status 0.164\n",
      "file 0.147\n",
      "appropriately 0.142\n"
     ]
    }
   ],
   "source": [
    "idx=120\n",
    "keywords=get_keywords(idx)\n",
    "print_results(idx,keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A text sample about keyword extraction description, slightly out of the training domain (stackoverflow) to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Raw Text=====\n",
      "\n",
      "Keyword extraction allows business to sift through big data to capture the most important words that best \n",
      "describe the text (e.g. customer review) in just seconds, obtain insights about the topics that your customers are \n",
      "talking about while saving your teams many hours of manual processing. \n",
      "It also provides you with actionable insights that you can use to make better business decisions.\n",
      "The best thing about keyword extraction models is that they are easy to set up and implement.\n",
      "Keyword extraction can help you obtain the most important keywords or key phrases from a given text without having to actually read a single line.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "extraction 0.478\n",
      "keyword 0.367\n",
      "insights 0.309\n",
      "business 0.241\n",
      "obtain 0.237\n",
      "important 0.222\n",
      "best 0.167\n",
      "decisions 0.164\n",
      "teams 0.155\n",
      "topics 0.139\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once\n",
    "#feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "#doc=docs_test[0] # a list of text strings\n",
    "doc = myText #the orginal text sample above \n",
    "\n",
    "#generate tf-idf for the given document - a key command to calculate tf-idf! \n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "#keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "keywords=extract_topn_from_vector2(feature_names,sorted_items,10)\n",
    "# now print the results\n",
    "print(\"\\n=====Raw Text=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another ODA text sample out of the training domain (stackoverflow) to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Raw Text=====\n",
      "Open Digital Architecture. ODA transforms business agility, \n",
      "customer experience and operational efficiency \n",
      "by creating simpler IT solutions \n",
      "that are easier and cheaper to deploy, integrate & upgrade.\n",
      "\n",
      "===Keywords===\n",
      "transforms 0.339\n",
      "digital 0.324\n",
      "efficiency 0.314\n",
      "simpler 0.29\n",
      "architecture 0.266\n",
      "upgrade 0.261\n",
      "integrate 0.26\n",
      "business 0.254\n",
      "deploy 0.241\n",
      "easier 0.238\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once\n",
    "#feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "#doc=docs_test[0] # a list of text strings\n",
    "doc = '''Open Digital Architecture. ODA transforms business agility, \n",
    "customer experience and operational efficiency \n",
    "by creating simpler IT solutions \n",
    "that are easier and cheaper to deploy, integrate & upgrade.''' #the TMForum ODA description\n",
    "\n",
    "#generate tf-idf for the given document - a key command to calculate tf-idf! \n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "#keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "keywords=extract_topn_from_vector2(feature_names,sorted_items,10)\n",
    "# now print the results\n",
    "print(\"\\n=====Raw Text=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "Good, but not perfect yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's enhance a bit Sklearn TF-IDF method by enabling n-gram support:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Option A:  remove frequent words and stop-words\n",
    "\n",
    "# Combining CountVectorizer and ngrams in Python: https://stackoverflow.com/questions/47887247/combining-countvectorizer-and-ngrams-in-python\n",
    "# simply add ngram_range to CountVectorizer constructor, e.g. ngram_range=(1,3) means we wants uigram, bigram and trigram\n",
    "#cv=CountVectorizer(ngram_range=(1,2),max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "cv=CountVectorizer(ngram_range=(1,2),max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "print(word_count_vector.shape)\n",
    "list(cv.get_feature_names())[2000:2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1184760)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_click define',\n",
       " '_click dim',\n",
       " '_click gt',\n",
       " '_click here',\n",
       " '_click object',\n",
       " '_click only',\n",
       " '_click runat',\n",
       " '_click sender',\n",
       " '_click sub',\n",
       " '_click system',\n",
       " '_click text',\n",
       " '_click this',\n",
       " '_click_',\n",
       " '_click_ sender',\n",
       " '_client']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option B:  NOT remove frequent words and stop-words\n",
    "\n",
    "# Combining CountVectorizer and ngrams in Python: https://stackoverflow.com/questions/47887247/combining-countvectorizer-and-ngrams-in-python\n",
    "# simply add ngram_range to CountVectorizer constructor, e.g. ngram_range=(1,3) means we wants uigram, bigram and trigram\n",
    "#cv=CountVectorizer(ngram_range=(1,2),max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "cv=CountVectorizer(ngram_range=(1,2)) #enable unigram and bigram\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "print(word_count_vector.shape)\n",
    "list(cv.get_feature_names())[2000:2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Again, re-train tfidf transformer as tf matrix becomes different:\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Domain testing case for tf-idf method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "SQL Import Wizard - Error\n",
      "\n",
      "=====Body=====\n",
      "<p>I have a CSV file that I'm trying to import into SQL Management Server Studio.</p>\n",
      "\n",
      "<p>In Excel, the column giving me trouble looks like this:\n",
      "<a href=\"https://i.stack.imgur.com/pm0uS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pm0uS.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>Tasks > import data > Flat Source File > select file</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/G4b6I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/G4b6I.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>I set the data type for this column to DT_NUMERIC, adjust the DataScale to 2 in order to get 2 decimal places, but when I click over to Preview, I see that it's clearly not recognizing the numbers appropriately:</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/NZhiQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NZhiQ.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "<p>The column mapping for this column is set to type = decimal; precision 18; scale 2.</p>\n",
      "\n",
      "<p>Error message: Data Flow Task 1: Data conversion failed. The data conversion for column \"Amount\" returned status value 2 and status text \"The value could not be converted because of a potential loss of data.\".\n",
      " (SQL Server Import and Export Wizard)</p>\n",
      "\n",
      "<p>Can someone identify where I'm going wrong here?  Thanks!</p>\n",
      "\n",
      "===Keywords===\n",
      "data conversion 0.206\n",
      "column 0.202\n",
      "this column 0.16\n",
      "import 0.158\n",
      "data 0.157\n",
      "wizard 0.149\n",
      "decimal 0.126\n",
      "conversion 0.124\n",
      "the column 0.122\n",
      "sql 0.12\n"
     ]
    }
   ],
   "source": [
    "# Re-do the test \n",
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "idx=120 #to (random) pick a stackoverflow testing sample\n",
    "keywords=get_keywords(idx)\n",
    "print_results(idx,keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "It now wisely picked up the most important information about this issue i.e. **'data conversion'**, also 'sql', 'import', 'column' and 'decimal' are relevant. Good job of tf-idf method with n-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Integrate War-Plugin for m2eclipse into Eclipse Project\n",
      "\n",
      "=====Body=====\n",
      "<p>I set up a small web project with JSF and Maven. Now I want to deploy on a Tomcat server. Is there a possibility to automate that like a button in Eclipse that automatically deploys the project to Tomcat?</p>\n",
      "\n",
      "<p>I read about a the <a href=\"http://maven.apache.org/plugins/maven-war-plugin/\" rel=\"nofollow noreferrer\">Maven War Plugin</a> but I couldn't find a tutorial how to integrate that into my process (eclipse/m2eclipse).</p>\n",
      "\n",
      "<p>Can you link me to help or try to explain it. Thanks.</p>\n",
      "\n",
      "===Keywords===\n",
      "eclipse 0.342\n",
      "war plugin 0.232\n",
      "war 0.183\n",
      "integrate 0.162\n",
      "maven 0.157\n",
      "tomcat 0.156\n",
      "project 0.138\n",
      "my process 0.13\n",
      "you link 0.125\n",
      "tutorial how 0.125\n"
     ]
    }
   ],
   "source": [
    "#another in-domain testing case\n",
    "idx=0#to (random) pick a stackoverflow testing sample\n",
    "keywords=get_keywords(idx)\n",
    "print_results(idx,keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "Again, n-gram tf-idf is capable of picking up the most important information about this issue i.e. **'war plugin'**, also 'eclipse', 'maven', 'tomcat' and 'project' are relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Domain testing case (neuroscience) for tf-idf method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Raw Text=====\n",
      "NEUROSCIENCE CORE CONCEPTS\n",
      "The\tbrain\tis\tthe\tbody's\tmost\tcomplex\torgan.\n",
      "Genetically\tdetermined\tcircuits\tare\tthe\tfoundation\tof\tthe\tnervous\tsystem.\n",
      "The\thuman\tbrain\tendows\tus\twith\ta\tnatural\tcuriosity\tto understand\thow\tthe\tworld\tworks.\n",
      "Fundamental\tdiscoveries\tpromote\thealthy\tliving\tand\ttreatment\tof\tdisease.\n",
      "\n",
      "===Keywords===\n",
      "brain 0.331\n",
      "us with 0.209\n",
      "the human 0.209\n",
      "most complex 0.209\n",
      "foundation of 0.209\n",
      "nervous 0.201\n",
      "the foundation 0.195\n",
      "promote 0.195\n",
      "living 0.195\n",
      "disease 0.195\n"
     ]
    }
   ],
   "source": [
    "# Re-do the test \n",
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "#print(feature_names[10000:10050])\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "#doc=docs_test[0] # a list of text strings\n",
    "doc = '''NEUROSCIENCE CORE CONCEPTS\n",
    "The\tbrain\tis\tthe\tbody's\tmost\tcomplex\torgan.\n",
    "Genetically\tdetermined\tcircuits\tare\tthe\tfoundation\tof\tthe\tnervous\tsystem.\n",
    "The\thuman\tbrain\tendows\tus\twith\ta\tnatural\tcuriosity\tto understand\thow\tthe\tworld\tworks.\n",
    "Fundamental\tdiscoveries\tpromote\thealthy\tliving\tand\ttreatment\tof\tdisease.'''\n",
    "\n",
    "#generate tf-idf for the given document - a key command to calculate tf-idf! \n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "#keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "top_n=10\n",
    "keywords=extract_topn_from_vector2(feature_names,sorted_items,top_n)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Raw Text=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "Fair result: for out-of-domain test, the tf-idf model trained on stackoverflow domain data can still pick up some keywords in neuroscience domain like 'brain' and 'the human' but not others like 'neuroscience' and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Scikit-Learn with TF-IDF for Keyword Extraction:  \n",
    "- The result is promising: to some degree, the model is **able to generise** for new text out of the training domain. But in principle you should ensure the test samples to predict should fall in the same/similar domain as the training data to make tf-idf method work properly.  \n",
    "- Single keyword (unigram) is extracted, but not key phrases as RAKE-NLTK does by default above. When I enabled n-gram it can give some keyphrases like 'the human' above, but key keyphrases such as 'nervous system' are still missing ('cos such phrase did not exist in the training data at all!)  \n",
    "- This proves from another angle how heavily TF-IDF approach accuracy depends on the relevance and representativeness of training text data to the target sample to predict   \n",
    "- Tentative conclusion: \n",
    "**highly customible and tunable, excellent for in-domain prediction but it requires big data to be able to generalise well and requires trial/error to fine tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoala! Now you can extract important keywords from any type of text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PKE - Python Keyword Extraction      \n",
    "- pke is an open source Python-based keyphrase extraction toolkit.  \n",
    "- pke provides a standardized **API** for extracting keyphrases from a document. **Inputs can be file or text string.**  \n",
    "- pke uses NLTK and SpaCy under the hood \n",
    "- Tutorials and code documentation are available at https://boudinfl.github.io/pke/.  \n",
    "- It provides **an end-to-end keyphrase extraction pipeline** in which each component can be easily modified or extended to develop new models. pke also allows for easy benchmarking of state-of-the-art keyphrase extraction models, and ships with supervised models trained on the SemEval-2010 dataset (https://www.aclweb.org/anthology/S10-1004/).   \n",
    "- Start by typing the 5 lines below. For using another model, simply replace pke.unsupervised.TopicRank with another model (a list of implemented models: https://github.com/boudinfl/pke#implemented-models).  \n",
    "- Implemented models  \n",
    "pke currently implements the following keyphrase extraction models:  \n",
    "\n",
    "- **Unsupervised models**  \n",
    "-- **Statistical models**  \n",
    "--- TfIdf [documentation]  \n",
    "--- KPMiner [documentation, article by (El-Beltagy and Rafea, 2010)]  \n",
    "--- YAKE [documentation, article by (Campos et al., 2020)]  \n",
    "-- **Graph-based models**  \n",
    "--- TextRank [documentation, article by (Mihalcea and Tarau, 2004)]  \n",
    "--- SingleRank [documentation, article by (Wan and Xiao, 2008)]  \n",
    "--- TopicRank [documentation, article by (Bougouin et al., 2013)]  \n",
    "--- TopicalPageRank [documentation, article by (Sterckx et al., 2015)]  \n",
    "--- PositionRank [documentation, article by (Florescu and Caragea, 2017)]  \n",
    "--- MultipartiteRank [documentation, article by (Boudin, 2018)]  \n",
    "- **Supervised models**  \n",
    "-- **Feature-based models**  \n",
    "--- Kea [documentation, article by (Witten et al., 2005)]  \n",
    "--- WINGNUS [documentation, article by (Nguyen and Luong, 2010)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation \n",
    "# pip install git+https://github.com/boudinfl/pke.git\n",
    "# install following in a cmd terminal \n",
    "# python -m nltk.downloader stopwords\n",
    "# python -m nltk.downloader universal_tagset\n",
    "# python -m spacy download en # download the english model (note: You can now load the model via spacy.load('en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank() \n",
    "#extractor = pke.unsupervised.MultipartiteRank() #also good, others may need to customise their hyperparameters? \n",
    "#extractor = pke.supervised.WINGNUS() #Kea() #not good\n",
    "\n",
    "# load the content of the document, here document is expected to be in raw\n",
    "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
    "#extractor.load_document(input='/path/to/input.txt', language='en')\n",
    "extractor.load_document(input='./sample_text.txt', language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm by default \n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyword extraction', 0.2653032756303063),\n",
       " ('text', 0.10994610654811321),\n",
       " ('key phrases', 0.09800165682226829),\n",
       " ('business', 0.0929954274390085),\n",
       " ('best thing', 0.08786144700445668),\n",
       " ('seconds', 0.0790037656731154),\n",
       " ('insights', 0.07613533455514761),\n",
       " ('easy', 0.07264997703427613),\n",
       " ('big data', 0.07073413624959467),\n",
       " ('manual processing', 0.047368873043712914)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text sample:\n",
      "integrate war plugin for m eclipse into eclipse project i set up a small web project with jsf and maven now i want to deploy on a tomcat server is there a possibility to automate that like a button in eclipse that automatically deploys the project to tomcat i read about a the maven war plugin but i couldn t find a tutorial how to integrate that into my process eclipse m eclipse can you link me to help or try to explain it thanks \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('small web project', 0.16383828363291691),\n",
       " ('eclipse', 0.15532567270795175),\n",
       " ('maven', 0.15244269621720916),\n",
       " ('jsf', 0.12483748240736106),\n",
       " ('tomcat server', 0.08416212975417435),\n",
       " ('possibility', 0.08249195204917972),\n",
       " ('button', 0.0726443762532862),\n",
       " ('war', 0.0708371213170549),\n",
       " ('tutorial', 0.05479219595990904),\n",
       " ('thanks', 0.038628089700957394)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another zero-shot testing case:\n",
    "idx=0 # 120\n",
    "print(f'text sample:\\n{docs_test[idx] }')\n",
    "\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank() \n",
    "#extractor = pke.unsupervised.MultipartiteRank() #also good, others may need to customise their hyperparameters? \n",
    "#extractor = pke.supervised.WINGNUS() #Kea() #not good\n",
    "\n",
    "# load the content of the document, here document is expected to be in raw\n",
    "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
    "#extractor.load_document(input='/path/to/input.txt', language='en')\n",
    "extractor.load_document(input=docs_test[idx], language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE Prediction Results Observation\n",
    "PKE is nice, easy and pretty accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashText      \n",
    "- This module can be used to replace keywords in sentences or extract keywords from sentences. It is based on the FlashText algorithm (https://arxiv.org/abs/1711.00046).  \n",
    "- Github Repo: https://github.com/vi3k6i5/flashtext  \n",
    "- Documentation can be found at (https://flashtext.readthedocs.io/en/latest/)  \n",
    "- Input is text string, also (drawback): you have to supply a pre-defined set of keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation \n",
    "# pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Keywords Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York', 'Bay Area', 'Action Point']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_processor = KeywordProcessor()\n",
    "# keyword_processor.add_keyword(<unclean name>, <standardised name>)\n",
    "keyword_processor.add_keyword('Big Apple', 'New York') # case insensitive, input is a tuple of (informal_name, associated_formal_name)\n",
    "keyword_processor.add_keyword('Bay Area')\n",
    "keyword_processor.add_keyword('action', 'Action Point')\n",
    "keywords_found = keyword_processor.extract_keywords('I love Big Apple and Bay Area. My aCtION is to complete a draft')\n",
    "keywords_found\n",
    "# ['New York', 'Bay Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_processor.add_keyword('Keyword Extraction')\n",
    "keyword_processor.add_keyword('Business Decision')\n",
    "keyword_processor.add_keyword('Business Decisions')\n",
    "keyword_processor.add_keyword('Business Value')\n",
    "keyword_processor.add_keyword('Business Values')\n",
    "keyword_processor.add_keyword('Business Intelligence')\n",
    "keyword_processor.add_keyword('Business Analytics')\n",
    "keyword_processor.add_keyword('Business Analysis')\n",
    "keyword_processor.add_keyword('Insight')\n",
    "keyword_processor.add_keyword('Insights')\n",
    "keyword_processor.add_keyword('Actionable Insight')\n",
    "keyword_processor.add_keyword('Actionable Insights')\n",
    "keyword_processor.add_keyword('Use Case')\n",
    "keyword_processor.add_keyword('Use Cases')\n",
    "keyword_processor.add_keyword('Business Scenario')\n",
    "keyword_processor.add_keyword('Business Scenarios')\n",
    "keyword_processor.add_keyword('Data-driven')\n",
    "keyword_processor.add_keyword('Data-driven Decision')\n",
    "keyword_processor.add_keyword('Data-driven Decisions')\n",
    "keyword_processor.add_keyword('Data-driven Strategy')\n",
    "keyword_processor.add_keyword('Data-driven Strategies')\n",
    "keyword_processor.add_keyword('Manual')\n",
    "keyword_processor.add_keyword('Automate')\n",
    "keyword_processor.add_keyword('Automation')\n",
    "keyword_processor.add_keyword('Save Time')\n",
    "keyword_processor.add_keyword('Manaul Work')\n",
    "keyword_processor.add_keyword('Efficient')\n",
    "keyword_processor.add_keyword('Efficiency')\n",
    "keyword_processor.add_keyword('Productivity')\n",
    "keyword_processor.add_keyword('KPI')\n",
    "keyword_processor.add_keyword('KPIs')\n",
    "keyword_processor.add_keyword('Speed')\n",
    "keyword_processor.add_keyword('Speed To Market')\n",
    "keyword_processor.add_keyword('Simplicity')\n",
    "keyword_processor.add_keyword('Simplicify')\n",
    "keyword_processor.add_keyword('Simplicification')\n",
    "keyword_processor.add_keyword('Radical Simplicity')\n",
    "keyword_processor.add_keyword('TCO')\n",
    "keyword_processor.add_keyword('Total Cost of Ownership')\n",
    "keyword_processor.add_keyword('Local Market')\n",
    "keyword_processor.add_keyword('LM')\n",
    "keyword_processor.add_keyword('OpCo')\n",
    "keyword_processor.add_keyword('Big Four')\n",
    "keyword_processor.add_keyword('Big 4')\n",
    "keyword_processor.add_keyword('Customer-oriented')\n",
    "keyword_processor.add_keyword('Customer Experience')\n",
    "keyword_processor.add_keyword('Customer Experiences')\n",
    "keyword_processor.add_keyword('Customer Support')\n",
    "keyword_processor.add_keyword('Customer Service')\n",
    "keyword_processor.add_keyword('Customer Support Ticket')\n",
    "keyword_processor.add_keyword('Customer Feedback')\n",
    "keyword_processor.add_keyword('Customer Survey')\n",
    "keyword_processor.add_keyword('Customer Review')\n",
    "keyword_processor.add_keyword('Churn')\n",
    "keyword_processor.add_keyword('Churn Rate')\n",
    "keyword_processor.add_keyword('NPS')\n",
    "keyword_processor.add_keyword('Strategy')\n",
    "keyword_processor.add_keyword('Data-driven Strategy')\n",
    "keyword_processor.add_keyword('Tech2025')\n",
    "keyword_processor.add_keyword('Tech Company')\n",
    "keyword_processor.add_keyword('Techy Company')\n",
    "keyword_processor.add_keyword('Telco Company')\n",
    "keyword_processor.add_keyword('Tecol')\n",
    "keyword_processor.add_keyword('Tecols')\n",
    "keyword_processor.add_keyword('Big Data')\n",
    "keyword_processor.add_keyword('Software')\n",
    "keyword_processor.add_keyword('Machine Learning')\n",
    "keyword_processor.add_keyword('AI')\n",
    "keyword_processor.add_keyword('Artificial Intelligence')\n",
    "keyword_processor.add_keyword('GCP')\n",
    "keyword_processor.add_keyword('Google Cloud Platform')\n",
    "keyword_processor.add_keyword('Vendor')\n",
    "keyword_processor.add_keyword('Vendor Management')\n",
    "keyword_processor.add_keyword('Supply Chain')\n",
    "keyword_processor.add_keyword('Social Media')\n",
    "keyword_processor.add_keyword('Brand')\n",
    "keyword_processor.add_keyword('Image')\n",
    "keyword_processor.add_keyword('Reputation')\n",
    "keyword_processor.add_keyword('Analytics')\n",
    "keyword_processor.add_keyword('Analytics CoE')\n",
    "keyword_processor.add_keyword('Manual Processing')\n",
    "keyword_processor.add_keyword('Easy to Use')\n",
    "keyword_processor.add_keyword('Easy to Set Up')\n",
    "keyword_processor.add_keyword('Seconds')\n",
    "keyword_processor.add_keyword('Minutes')\n",
    "keyword_processor.add_keyword('Hours')\n",
    "keyword_processor.add_keyword('Days')\n",
    "keyword_processor.add_keyword('Weeks')\n",
    "keyword_processor.add_keyword('Months')\n",
    "keyword_processor.add_keyword('Years')\n",
    "keyword_processor.add_keyword('Delay')\n",
    "keyword_processor.add_keyword('Latency')\n",
    "keyword_processor.add_keyword('Reliable')\n",
    "keyword_processor.add_keyword('Reliability')\n",
    "keyword_processor.add_keyword('Security')\n",
    "keyword_processor.add_keyword('Flexibility')\n",
    "keyword_processor.add_keyword('Flexible')\n",
    "keyword_processor.add_keyword('Social Media')\n",
    "keyword_processor.add_keyword('False Information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"keyword extraction allows business to sift through big data to capture the most important words that best\"\n",
      "\"describe the text (e.g. customer review) in just seconds, obtain insights about the topics that your customers are\" \n",
      "\"talking about while saving your teams many hours of manual processing. \"\n",
      "\"it also provides you with actionable insights that you can use to make better business decisions.\"\n",
      "\"the best thing about keyword extraction models is that they are easy to set up and implement.\"\n",
      "\"keyword extraction can help you obtain the most important keywords or key phrases from a given text without having to\"\n",
      "\"actually read a single line.\"\n",
      "\"social media giants like google does that.\"\n",
      "\"social media giants like facebook are under increasing pressure to stop the spread of false information\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Actionable Insights',\n",
       " 'Big Data',\n",
       " 'Business Decisions',\n",
       " 'Customer Review',\n",
       " 'Easy to Set Up',\n",
       " 'False Information',\n",
       " 'Hours',\n",
       " 'Insights',\n",
       " 'Keyword Extraction',\n",
       " 'Manual Processing',\n",
       " 'Seconds',\n",
       " 'Social Media'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str= '''\"Keyword extraction allows business to sift through big data to capture the most important words that best\"\n",
    "\"describe the text (e.g. customer review) in just seconds, obtain insights about the topics that your customers are\" \n",
    "\"talking about while saving your teams many hours of manual processing. \"\n",
    "\"It also provides you with actionable insights that you can use to make better business decisions.\"\n",
    "\"The best thing about keyword extraction models is that they are easy to set up and implement.\"\n",
    "\"Keyword extraction can help you obtain the most important keywords or key phrases from a given text without having to\"\n",
    "\"actually read a single line.\"\n",
    "\"Social Media giants like google does that.\"\n",
    "\"Social media giants like Facebook are under increasing pressure to stop the spread of false information\"'''\n",
    "test_str_lower = test_str.lower()\n",
    "print(test_str_lower)\n",
    "\n",
    "keywords_found = keyword_processor.extract_keywords(test_str.lower())\n",
    "set(keywords_found)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of FlashText \n",
    "1. Pros: highly flexible and can self define a custom keywords list  \n",
    "2. Cons: manual pre-definition, and inputs only accept text string, not file  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative Assessment of Keyword Extraction Tools  \n",
    "#### 1. PKE  \n",
    "- Pros: 'Best of Breed', accurate, powerful with many algos support, support both file/string inputs, easy to use, generalise well for zero-shot prediction for most use cases!     \n",
    "- Cons: still searching... :-D    \n",
    "\n",
    "#### 2. RAKE-NLTK   \n",
    "- Pros: pretty accurate, automatic and easy to use  \n",
    "- Cons: Result is a bit verbose (phrase max_length is set high)  \n",
    "\n",
    "#### 3. FlashText  \n",
    "- Pros: highly customisable, accurate and optimised to specific domain vocabulary  \n",
    "- Cons: need to manually define a custom keyword look-up list     \n",
    "\n",
    "#### 4. Scikit-Learn TF-IDF  \n",
    "- Pros: highly tunable, excellent for in-domain prediction  \n",
    "- Cons: it requires big data to be able to generalise and requires manual work, i.e. its quality depends on training data (require relevant and representative documents for training a good model for testing domain) \n",
    "\n",
    "#### Summary\n",
    "If you got a test text from which you'd like to extract keywords but have no idea where to start with, try **PKE** first as it zero-shot prediction ability often can deliver good/excellent results in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
